{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools or clicking in 'Inspect' on any browser. Here is an example:\n",
    "\n",
    "![title](example_1.png)\n",
    "\n",
    "2. Use BeautifulSoup `find_all()` to extract all the html elements that contain the developer names. Hint: pass in the `attrs` parameter to specify the class.\n",
    "\n",
    "3. Loop through the elements found and get the text for each of them.\n",
    "\n",
    "4. While you are at it, use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names. Hint: you may also use `.get_text()` instead of `.text` and pass in the desired parameters to do some string manipulation (check the documentation).\n",
    "\n",
    "5. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MichaIng', 'Brandon', 'Arvid Norberg', 'Adrienne Walker', 'Artem Zakharchenko', 'Ben Johnson', 'Jonny Burger', 'David Khourshid', 'andig', 'Sebastian Pipping', 'Sam Sam', 'Andrew Gallant', 'Kévin Dunglas', 'Vincent Prouillet', 'Joshua Wise', 'Jaime Blasco', 'Nelson Elhage', 'Joe Previte', 'Adam Bieńkowski', 'Jbee', 'Franck Nijhof', 'lsvih', 'Eldad A. Fux', 'Christopher Rackauckas', 'Nicolas P. Rougier']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# retrieve ids of trending developers\n",
    "ids = [item[\"id\"] for item in soup.find_all(\"article\", class_=\"Box-row d-flex\")]\n",
    "\n",
    "# get the user_name from the id\n",
    "user_names = [id_.replace(\"pa-\", \"\") for id_ in ids] \n",
    "\n",
    "# use user_names to retrieve full names\n",
    "full_names = [[full_name.text.strip() for full_name in soup.find_all(\"a\", href=\"/\" + user_name)][1] for user_name in user_names]\n",
    "\n",
    "# print list of full names\n",
    "print(full_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "# changing url slightly to get all langs\n",
    "#url = \"https://github.com/trending/python?since=daily&spoken_language_code=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system-design-primer', 'Binance-volatility-trading-bot', 'fairseq', 'Python', 'core', 'rich', 'backtrader', 'binance-trade-bot', 'vaccipy', 'genshinhelper', 'Ultimate-Python-Resource-Hub', 'covid-vaccine-booking', 'anki', 'JD-Script', 'scikit-learn', 'voice2json', 'bitcoinbook', 'public-apis', 'Profil3r', 'charlotte', 'chia-blockchain', 'RsaCtfTool', 'Ultroid', 'python-cheatsheet', 'U-2-Net']\n"
     ]
    }
   ],
   "source": [
    "# get the owner and the repo\n",
    "owner_repo = [repo.text.strip().replace(\"\\n\", \"\").replace(\" \", \"\") for repo in soup.find_all(\"h1\", class_=\"h3 lh-condensed\")]\n",
    "\n",
    "# get list of repos\n",
    "repos = [item.split(\"/\")[1] for item in owner_repo]\n",
    "\n",
    "# print list\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display all the image links from Walt Disney wikipedia page.\n",
    "Hint: use `.get()` to access information inside tags. Check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1', 'https:/static/images/footer/wikimedia-button.png', 'https:/static/images/footer/poweredby_mediawiki_88x31.png']\n"
     ]
    }
   ],
   "source": [
    "# get links to all images\n",
    "image_links = [(\"https:\" + item[\"src\"]) for item in soup.find_all(\"img\")]\n",
    "\n",
    "# print the list of links\n",
    "print(image_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'language': 'English', 'num_articles': '6299000+ articles'}, {'language': '日本語', 'num_articles': '1268000+ 記事'}, {'language': 'Español', 'num_articles': '1684000+ artículos'}, {'language': 'Deutsch', 'num_articles': '2576000+ Artikel'}, {'language': 'Русский', 'num_articles': '1724000+ статей'}, {'language': 'Français', 'num_articles': '2329000+ articles'}, {'language': 'Italiano', 'num_articles': '1693000+ voci'}, {'language': '中文', 'num_articles': '1197000+ 條目'}, {'language': 'Português', 'num_articles': '1066000+ artigos'}, {'language': 'Polski', 'num_articles': '1473000+ haseł'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get languages and number of articles\n",
    "\n",
    "# create empty list\n",
    "langs_articles = []\n",
    "\n",
    "# loop the 10 languages\n",
    "for i in range(1,11):\n",
    "    \n",
    "    # get language\n",
    "    language = soup.find(\"div\", class_=\"central-featured-lang lang\" + str(i)).text.split(\"\\n\")[2]\n",
    "\n",
    "    # get num of articles\n",
    "    num_articles = soup.find(\"div\", class_=\"central-featured-lang lang\" + str(i)).text.split(\"\\n\")[3].replace(\"\\xa0\", \"\")\n",
    "\n",
    "    # append to empty list\n",
    "    langs_articles.append({\"language\": language, \"num_articles\": num_articles})\n",
    "    \n",
    "# print list\n",
    "print(langs_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Display the top 10 languages by number of native speakers stored in a pandas dataframe.\n",
    "Hint: After finding the correct table you want to analyse, you can use a nested **for** loop to find the elements row by row (check out the 'td' and 'tr' tags). <br>An easier way to do it is using pd.read_html(), check out documentation [here](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Mandarin Chinese\n",
      "1                               Spanish\n",
      "2                               English\n",
      "3    Hindi (sanskritised Hindustani)[9]\n",
      "4                               Bengali\n",
      "5                            Portuguese\n",
      "6                               Russian\n",
      "7                              Japanese\n",
      "8                   Western Punjabi[10]\n",
      "9                               Marathi\n",
      "Name: Language, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#soup.find_all(\"table\", class_=\"wikitable sortable\")\n",
    "# find table\n",
    "table = (soup.find(\"table\", class_=\"wikitable sortable\"))\n",
    "\n",
    "#read html\n",
    "langs = pd.read_html(str(table))[0]\n",
    "\n",
    "# print list of 10 langs\n",
    "print(langs[\"Language\"][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe.\n",
    "Hint: If you hover over the title of the movie, you should see the director's name. Can you find where it's stored in the html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_year</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os Condenados de Shawshank</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>[ Tim Robbins,  Morgan Freeman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Padrinho</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[ Marlon Brando,  Al Pacino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O Padrinho: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[ Al Pacino,  Robert De Niro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Cavaleiro das Trevas</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>[ Christian Bale,  Heath Ledger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doze Homens em Fúria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>[ Henry Fonda,  Lee J. Cobb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Shin seiki Evangelion Gekijô-ban: Air/Magoko...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Hideaki Anno</td>\n",
       "      <td>[ Megumi Ogata,  Megumi Hayashibara]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Anand</td>\n",
       "      <td>1971</td>\n",
       "      <td>Hrishikesh Mukherjee</td>\n",
       "      <td>[ Rajesh Khanna,  Amitabh Bachchan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>O Homem Que Matou Liberty Valance</td>\n",
       "      <td>1962</td>\n",
       "      <td>John Ford</td>\n",
       "      <td>[ James Stewart,  John Wayne]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>2013</td>\n",
       "      <td>Jeethu Joseph</td>\n",
       "      <td>[ Mohanlal,  Meena]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Paris, Texas</td>\n",
       "      <td>1984</td>\n",
       "      <td>Wim Wenders</td>\n",
       "      <td>[ Harry Dean Stanton,  Nastassja Kinski]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title release_year  \\\n",
       "0                          Os Condenados de Shawshank          1994   \n",
       "1                                          O Padrinho          1972   \n",
       "2                                O Padrinho: Parte II          1974   \n",
       "3                              O Cavaleiro das Trevas          2008   \n",
       "4                                Doze Homens em Fúria          1957   \n",
       "..                                                 ...          ...   \n",
       "245    Shin seiki Evangelion Gekijô-ban: Air/Magoko...         1997   \n",
       "246                                             Anand          1971   \n",
       "247                 O Homem Que Matou Liberty Valance          1962   \n",
       "248                                          Drishyam          2013   \n",
       "249                                      Paris, Texas          1984   \n",
       "\n",
       "                  director                                     stars  \n",
       "0          Frank Darabont            [ Tim Robbins,  Morgan Freeman]  \n",
       "1    Francis Ford Coppola               [ Marlon Brando,  Al Pacino]  \n",
       "2    Francis Ford Coppola              [ Al Pacino,  Robert De Niro]  \n",
       "3       Christopher Nolan           [ Christian Bale,  Heath Ledger]  \n",
       "4            Sidney Lumet               [ Henry Fonda,  Lee J. Cobb]  \n",
       "..                     ...                                       ...  \n",
       "245          Hideaki Anno       [ Megumi Ogata,  Megumi Hayashibara]  \n",
       "246  Hrishikesh Mukherjee        [ Rajesh Khanna,  Amitabh Bachchan]  \n",
       "247             John Ford              [ James Stewart,  John Wayne]  \n",
       "248         Jeethu Joseph                        [ Mohanlal,  Meena]  \n",
       "249           Wim Wenders   [ Harry Dean Stanton,  Nastassja Kinski]  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find table\n",
    "table = soup.find(\"table\", attrs={\"data-caller-name\":\"chart-top250movie\"})\n",
    "\n",
    "# read html\n",
    "movies = pd.read_html(str(table))[0]\n",
    "\n",
    "# split into title and release year\n",
    "movies[['title','release_year']] = movies[\"Rank & Title\"].str.split(pat=\"(\", expand=True, n=1)\n",
    "\n",
    "# clean release year\n",
    "movies[\"release_year\"] = movies[\"release_year\"].str.replace(\")\", \"\", regex=False)\n",
    "\n",
    "# clean title\n",
    "movies[\"title\"] = movies[\"title\"].str.split(\".\", n=1, expand=True)[1]\n",
    "\n",
    "# choose columns to keep\n",
    "movies = movies[['title', 'release_year']]\n",
    "\n",
    "# empty lists for directors and stars\n",
    "directors = []\n",
    "stars = []\n",
    "\n",
    "# retrieve directors and stars\n",
    "for x in table.find_all(\"a\"):\n",
    "    try:\n",
    "        directors.append(x[\"title\"].split(\",\")[0])\n",
    "        stars.append(x[\"title\"].split(\",\")[1:])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# create columns for directors and stars\n",
    "movies[\"director\"] = directors\n",
    "movies[\"stars\"] = stars\n",
    "\n",
    "# clean director\n",
    "movies[\"director\"] = movies[\"director\"].str.replace(\"(dir.)\", \"\", regex=False)\n",
    "\n",
    "# display df\n",
    "display(movies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'https://www.imdb.com/list/ls009796553/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "content = response.content\n",
    "\n",
    "# Soup creation\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pesadelo em Elm Street</td>\n",
       "      <td>1984</td>\n",
       "      <td>[The monstrous spirit of a slain child murdere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despertares</td>\n",
       "      <td>1990</td>\n",
       "      <td>[The victims of an encephalitis epidemic many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liga de Mulheres</td>\n",
       "      <td>1992</td>\n",
       "      <td>[Two sisters join the first female professiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um Bairro em Nova Iorque</td>\n",
       "      <td>1993</td>\n",
       "      <td>[A father becomes worried when a local gangste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anjos em Campo</td>\n",
       "      <td>1994</td>\n",
       "      <td>[When a boy prays for a chance to have a famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tempo de Matar</td>\n",
       "      <td>1996</td>\n",
       "      <td>[In Canton, Mississippi, a fearless young lawy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amistad</td>\n",
       "      <td>1997</td>\n",
       "      <td>[In 1839, the revolt of Mende captives aboard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anaconda</td>\n",
       "      <td>1997</td>\n",
       "      <td>[A \"National Geographic\" film crew is taken ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Cool, Dry Place</td>\n",
       "      <td>1998</td>\n",
       "      <td>[Russell, single father balances his work as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>América Proibida</td>\n",
       "      <td>1998</td>\n",
       "      <td>[A former neo-nazi skinhead tries to prevent h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  year  \\\n",
       "0    Pesadelo em Elm Street  1984   \n",
       "1               Despertares  1990   \n",
       "2          Liga de Mulheres  1992   \n",
       "3  Um Bairro em Nova Iorque  1993   \n",
       "4            Anjos em Campo  1994   \n",
       "5            Tempo de Matar  1996   \n",
       "6                   Amistad  1997   \n",
       "7                  Anaconda  1997   \n",
       "8         A Cool, Dry Place  1998   \n",
       "9          América Proibida  1998   \n",
       "\n",
       "                                         description  \n",
       "0  [The monstrous spirit of a slain child murdere...  \n",
       "1  [The victims of an encephalitis epidemic many ...  \n",
       "2  [Two sisters join the first female professiona...  \n",
       "3  [A father becomes worried when a local gangste...  \n",
       "4  [When a boy prays for a chance to have a famil...  \n",
       "5  [In Canton, Mississippi, a fearless young lawy...  \n",
       "6  [In 1839, the revolt of Mende captives aboard ...  \n",
       "7  [A \"National Geographic\" film crew is taken ho...  \n",
       "8  [Russell, single father balances his work as a...  \n",
       "9  [A former neo-nazi skinhead tries to prevent h...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find titles\n",
    "titles = [[subitem.text for subitem in item.find_all(\"a\")][0] for item in soup.find_all(\"div\", class_=\"lister-item-content\")]\n",
    "\n",
    "# find years\n",
    "years = [item.text.replace(\"(\", \"\").replace(\")\", \"\") for item in soup.find_all(\"span\", class_= \"lister-item-year text-muted unbold\")]\n",
    "\n",
    "# find descriptions\n",
    "descriptions = [[subitem.text.replace(\"\\n\", \"\").strip() for subitem in item.find_all(\"p\", class_=\"\")] for item in soup.find_all(\"div\", class_= \"lister-item-content\")]\n",
    "\n",
    "# create df\n",
    "df = pd.DataFrame(list(zip(titles, years, descriptions)), columns =['title', 'year', \"description\"])\n",
    "\n",
    "# display top 10 in df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 100 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe.\n",
    "***Hint:*** Here the displayed number of earthquakes per page is 20, but you can easily move to the next page by looping through the desired number of pages and adding it to the end of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/?view='\n",
    "\n",
    "# This is how you will loop through each page:\n",
    "number_of_pages = int(100/20)\n",
    "each_page_urls = []\n",
    "\n",
    "for n in range(1, number_of_pages+1):\n",
    "    link = url+str(n)\n",
    "    each_page_urls.append(link)\n",
    "    \n",
    "each_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
